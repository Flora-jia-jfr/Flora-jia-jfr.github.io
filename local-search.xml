<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>CSCI499</title>
    <link href="/2022/10/17/CSCI499/"/>
    <url>/2022/10/17/CSCI499/</url>
    
    <content type="html"><![CDATA[<h1 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h1><h2 id="1-Sequence-to-sequence-models"><a href="#1-Sequence-to-sequence-models" class="headerlink" title="1. Sequence-to-sequence models"></a>1. Sequence-to-sequence models</h2><h2 id="2-Encoder-Decoder-attention"><a href="#2-Encoder-Decoder-attention" class="headerlink" title="2. Encoder-Decoder attention"></a>2. Encoder-Decoder attention</h2><h2 id="3-Transformer-networks"><a href="#3-Transformer-networks" class="headerlink" title="3. Transformer networks"></a>3. Transformer networks</h2>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World!</title>
    <link href="/2022/07/02/Hello_World/"/>
    <url>/2022/07/02/Hello_World/</url>
    
    <content type="html"><![CDATA[<h1 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World!"></a>Hello World!</h1><p>Finally have my personal website!!!<br>I would utilize this platform as a place to keep records of my progress along the way. </p><p><strong>Fight On!</strong></p>]]></content>
    
    
    <categories>
      
      <category>Life</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>MATH 545 Introduction to Time Series</title>
    <link href="/2022/02/26/MATH545_time_series/"/>
    <url>/2022/02/26/MATH545_time_series/</url>
    
    <content type="html"><![CDATA[<h1 id="MATH-545-Introduction-to-Time-Series"><a href="#MATH-545-Introduction-to-Time-Series" class="headerlink" title="MATH 545 Introduction to Time Series"></a>MATH 545 Introduction to Time Series</h1><h1 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h1><h3 id="Time-Series-Introduction"><a href="#Time-Series-Introduction" class="headerlink" title="Time Series Introduction"></a>Time Series Introduction</h3><p>Time series is not a series ‚Äî it‚Äôs a sequence</p><p>üí° A <strong>time series</strong> is a sequence of scalar or vector-valued observations recorded at ‚Äútimes‚Äù $t \in T$<br>In this course, $T \subset R$ ( other examples: $T &#x3D; \mathbb{N}, T &#x3D; S^2$)</p><p>We will model time series as a realization of some stochastic process {$X_t, t\in T$}</p><p>üí° A <strong>Stochastic process</strong> {$X_t, t \in T$} is a sequence of random variables defined on a probability space ($\Omega, F, P$) and indexed by a set $T$</p><p>Examples:</p><ol><li>$X_1, ‚Ä¶, X_n$ ‚Äî i.i.d. random variables</li><li>$S_n &#x3D; \sum_{j&#x3D;1}^n X_j, n &#x3D; 1,2,3,‚Ä¶$ ‚Äî a random walk {$S_n, n \in \mathbb{N}$}</li><li>The Brownian motion {${W_t, t\in [0,1]}$}<ol><li>$W_0$ &#x3D; 0</li><li>$\forall 0 &lt; t_1 &lt; t_2 &lt; ‚Ä¶ &lt; t_k, k \ge 3$, {$w_{t_2}-w_{t_1}, w_{t_3}-w_{t_2},‚Ä¶, w_{t_k}-w_{t_{k-1}}$} are independent, and {$w_t - w_s$} ~ $N(0, |t-s|)$</li></ol></li></ol><p>existence ‚Äî?</p><p>üí° Distribution function of $(x_{t_1}, ‚Ä¶ ,x_{t_k})$: ($k \ge 1, t_1, ‚Ä¶ t_k \in T$)<br>A stochastic process is characterized by its finite dimensional(fd) distribution $F_{t1, .. t_k}(x_1, ‚Ä¶ x_k) &#x3D; Pr(x_{t_1} \le x_1, ‚Ä¶ x_{t_k} \le x_k)$ [È´òÁª¥ÁöÑprobability density function]<br>In other words it‚Äôs a distribution function(d.f.) of the random vector $(x_{t_1}, ‚Ä¶ , x_{t_k}), k \ge 1, t_1, ‚Ä¶ t_k \in T$<br>{$F_{t_1, ‚Ä¶ ,t_k}, k \ge 1, t_1, ‚Ä¶,  t_k \in T$} is known as a family of finite dimensional distribution of {$x_t, t\in T$}</p><p><img src="/source/_posts/img/MATH545/Untitled.png" alt="Untitled"></p><p><img src="https://www.notion.so/icons/info-alternate_blue.svg" alt="https://www.notion.so/icons/info-alternate_blue.svg" width="40px" /> Theorem: (A.Kolmogorov, P.J.Damell)<br>Assume that {$F_{t_1, ‚Ä¶ ,t_k}, k \ge 1, t_1, ‚Ä¶,  t_k \in T$} is a family of probability measure such that<br>(a) if $\pi$ is any permutation $\pi:{ 1, ‚Ä¶, k} \rarr { 1, .. .,k}$ then $F_{t_{\pi(1)}, ‚Ä¶ t_{\pi(k)}} &#x3D; F_{t_1, ‚Ä¶ ,t_k}(x_1,‚Ä¶,x_k)$<br>(b) $\lim_{x_k \rarr \infin} F_{t_1, ‚Ä¶ t_k}(x_1, ‚Ä¶ , x_k) &#x3D; F_{t_1, ‚Ä¶ ,t_{k-1}}(x_1, ‚Ä¶, x_{k-1})$<br>Then there exists a stochastic process ${ X_t, t \in T}$ that has $F$ as its f.d. distribution (finite dimensional)</p><h3 id="Facts-about-Linear-Algebra"><a href="#Facts-about-Linear-Algebra" class="headerlink" title="Facts about Linear Algebra"></a>Facts about Linear Algebra</h3><p>let $A \in \mathbb{R}^{n \times n}$, then </p><ol><li><p>$A$ is symmetric if $A &#x3D; A^T$(equivalently $A_{ij} &#x3D; A_{ji}$ for all $i,j &#x3D; 1, ‚Ä¶ ,n$)</p></li><li><p>$A$ is nonnegative(positive) definite ‚Üî¬†$A$ is symmetric, and¬†$\forall x \in R^n, x\ne 0, \langle Ax, x \rangle &#x3D; x^TAx \ge 0 (&gt; 0)$</p><p> Fact: $A$ is symmetric</p><p> Reason: $x^TAx &#x3D; \sum_{i,j&#x3D;1}A_{ij}x_i x_j &#x3D; x^TA^Tx$ ‚áí $x^TAx &#x3D; x^T(\frac{A+A^T}{2})x$ since  $x^T(\frac{A+A^T}{2})x &#x3D; \sum_{i,j&#x3D;1} x_i \frac{A_{ij}+A_{ji}}{2}x_j$</p><p> <img src="/./img/MATH545/Untitled%201.png" alt="Untitled"></p><p> Fact: (Exercise)</p><p> $M \in C^{n \times n}$ is self-adjoint $M &#x3D;  \overline M^T$ ($\overline {x+iy} &#x3D; x-iy$)</p><p> If $\langle Mx, x \rangle \ge 0, \forall x\in C^n$ ‚áí $M$ is self-adjoint</p><p> Lemma: show that $\langle Mx, x \rangle &#x3D; 0, \forall x \in C^n$ ‚áí $M &#x3D; 0_{n \times n}$¬†($M &#x3D; 0$) </p><p> not true for $M \in R^{n \times n}$:  $M_{11}x_1^2+M_{22}x_2^2 +(M_{12}+M_{21})x_1x_2 &#x3D; 0 \forall x_1,x_2$</p></li></ol><p>A is nonnegative(positive definite) ‚Üî¬†A ‚â• 0 (A &gt; 0) </p><p>Properties: let $A \ge 0$ ($A \in R^{n \times n}$ be positive definite), then</p><ol><li><p>$A$ has n non-negative(positive) eigenvalues $\lambda_1, ‚Ä¶ ,\lambda_n$w, the corresponding eigenvectors $v_1, .. ,v_n$ form an orthonormal basis of $R^n$</p><p> Remainder: $\lambda$ is an eigenvalue with a corresponding eigenvector v ‚Üî¬†$Av &#x3D; \lambda v, v \ne 0$ </p><p> Â≠òÂú®‰∏Ä‰∏™ÁâπÂæÅÊ†πÂØπÂ∫îÂ§ö‰∏™ÁâπÂæÅÁü©ÈòµÁöÑÊÉÖÂÜµ</p></li><li><p>$A$ has an eigenvalue decomposition $A &#x3D; V \Lambda V^T$ where $V &#x3D; (v_1, ‚Ä¶ , V_n)$ and $\Lambda$ &#x3D; Âè™ÊúâÊñúËßíÊòØ $\lambda_1 ,‚Ä¶, \lambda_n$ ÁöÑÁü©Èòµ</p><p> Alternatively, $A$ can be represented as $A &#x3D; \sum_{j&#x3D;1}^n \lambda_j v_j v_j^T$</p><p> Exercise:</p><ol><li><p>$A \in R^{n \times n}$, $A &gt; 0$. Then $\langle x, y \rangle_A &#x3D; \langle Ax, y \rangle$ is an inner product</p><p> $\langle x,y \rangle_{I_n} &#x3D; \sum_{j&#x3D;1}^n x_j y_j$</p><p> $\langle x,y \rangle_{\Lambda} &#x3D; \sum_{j&#x3D;1}^n \lambda_j x_j y_j$ is an inner product ‚Üî¬†$\lambda_j &gt; 0, j &#x3D; 1, ‚Ä¶ , n$</p></li><li><p>the eigenvalue decomposition can be written as $A &#x3D; \sum_{j&#x3D;1}^n \lambda_j v_j v_j^T$</p></li><li><p>let $V$ be an orthogonal matrix, then $V^T &#x3D; V^{-1}$ and $V^T$ is also an orthogonal matrix</p></li></ol></li></ol><h3 id="Covariance-and-Correlation-matrices"><a href="#Covariance-and-Correlation-matrices" class="headerlink" title="Covariance and Correlation matrices"></a>Covariance and Correlation matrices</h3><p>let $Y &#x3D; (Y_1 ‚Ä¶ Y_n) \in R^n$, assume that $\mathbb{E}[Y_j^2] &lt; \infin$ for $j &#x3D; 1, 2, ‚Ä¶ , n$</p><p>In that follow, $\mathbb{E}[Y] &#x3D; (\mathbb{E}[Y_1] ‚Ä¶ \mathbb{E}[Y_n])$</p><p>The covariance matrix of $Y$ is defined via $\Sigma_Y &#x3D; \mathbb{E}[(Y-\mathbb{E}[Y])(Y-\mathbb{E}[Y])^T] \in R^{n \times n}$</p><blockquote><p>Covariance forumla: $Cov(X,Y) &#x3D; E[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]$</p></blockquote><p>In particular, $(\Sigma_Y)_{ij}&#x3D;\mathbb{E}[(Y_i-\mathbb{E}[Y_i])(Y_j-\mathbb{E}[Y_j])]  &#x3D;Cov(Y_i,Y_j)$</p><p>The correlation matrix is defined via $\rho_{i,j} &#x3D; Corr(Y_i, Y_j) &#x3D; \frac{Cov(Y_i, Y_j)}{\sqrt{Var(Y_i)Var(Y_j)}}$ if $Var(Y_1) \ne 0, Var(Y_2) \ne 0$, else 0</p><p>In particular, the correlation matrix $R_Y$ is the covariance matrix of $(\frac{Y_1}{\sqrt{Var(Y_1)}}, ‚Ä¶ ,\frac{Y_n}{\sqrt{Var(Y_n)}})$</p><blockquote><p>Cauchy Schwarz Inequality: $|\mathbb{E}[XY]| \le \sqrt{\mathbb{E}[X^2]\mathbb{E}[Y^2]}$<br>‚áí this is why we need to restriction of  $\mathbb{E}[Y_j^2] &lt; \infin$ above for the deduction and definition</p></blockquote><p>üí° Proposition: For any random vector $V \in R^n, \Sigma_V \ge 0$</p><p>Proof: Need to show that $x^T \Sigma_Y x \ge 0 \forall x \in R$</p><p>$$<br>x^T \Sigma_Y x &#x3D; x^T \mathbb{E}[(V-\mathbb{E}[V])(V-\mathbb{E}[V])^T]x<br>\ &#x3D; \mathbb{E}[x^T(V-\mathbb{E}[V])(V-\mathbb{E}[V])^Tx]<br>\ &#x3D; \mathbb{E}[\langle x, V-\mathbb{E}[V] \rangle^2]<br>\ \ge 0<br>$$</p><p>Proved!</p><blockquote><p>Remark: $\Sigma_Y &gt; 0$ unless the coordinates of $V$ are linearly dependent (that‚Äôs what happening when $\Sigma_Y &#x3D; 0$)</p></blockquote><h3 id="Multivariate-Normal-Distribution"><a href="#Multivariate-Normal-Distribution" class="headerlink" title="Multivariate Normal Distribution"></a>Multivariate Normal Distribution</h3><p>let $X$ be a r.v.</p><p>üí° Definition: Characteristic function (Fourier transformation of the density) of X is defined via $\phi_X(t) &#x3D; E[e^{itx}] &#x3D; E[cos(tx)] + i E[sin(tx)]$</p><p>If $X$ has pdf $p(x)$, then $\phi_x(t) &#x3D; \int_R e^{itx} p(x) dx$ (Fourier transformation of the density)</p></aside><p>Two Properties:</p><ol><li><p>$\phi_x(t)$ is well-defined: </p><p> $|\phi_x(t)| &#x3D; | \mathbb{E}[e^itx]| \le \mathbb{E}[e^itx] &#x3D; 1$</p><p> <img src="/./img/MATH545/Untitled%202.png" alt="on a complex plane"></p><p> on a complex plane</p></li><li><p>$\phi_x(t) &#x3D; \phi_y(t) \forall t \in R$ ‚Üî¬†Distributions of $X,Y$ coincide</p><p> characteristic function can exactly represent the distribution of a r.v. ‚Üî¬†If 2 r.v. have the same characteristic function, then the 2 r.v. have exactly the same distribution</p></li></ol><p>If $Y \in R^n$ is a random vector, we define $\phi_Y(t) &#x3D; \mathbb{E}[e^{i \langle t, Y \rangle}], t \in R^n &#x3D; \mathbb{E}[e^{i ||t||_2 \langle \frac{t}{||t||_2}, Y \rangle}]$ ‚Äî knowing distribution of all one-dimensional projections</p><aside>üí° Definition: $\vec Y \in R^n$ has standard multivariate normal distribution if $Y_1, ... , Y_n$ are i.i.d. $N(0,1)$ random variables<p>Remark: then $\Sigma_Y &#x3D; diag(1, ‚Ä¶ ,1)$ and all other are 0 matrices $&#x3D;I_n$</p></aside><aside>üí° Definition: $X \in R^k$ has multivariate normal distribution if $\exist a \in R^k$ and $b \in R^{k \times n}$ such that $X = a + bY$ where $Y \in R^n$ has standard multivariate normal distribution</aside><p>Properties:</p><ol><li><p>$\mathbb{E}[x] &#x3D; a$</p></li><li><p>$Cov(X) &#x3D; \Sigma_X &#x3D; \mathbb{E}[(X-a)(X-a)^T] &#x3D; \mathbb{E}[BY(BY)^T] &#x3D; \mathbb{E}[BYY^TB^T] &#x3D; BB^T$</p><p> reason for the last step: $Y$ is an orthogonal matrix (because $Y_1, ‚Ä¶ ,Y_n$ is i.i.d.)‚áí $YY^T &#x3D; I_n$ </p><p> $\Sigma_X &#x3D; BB^T &#x3D; BQQ^TB &#x3D; (BQ)(BQ)^T$ where $Q \in R^{n \times n}$ is any orthogonal matrix</p></li></ol><p>‚áí Notation: $X \sim N(a, \Sigma_X)$</p><ol><li><p>$\phi_X(t) &#x3D; \mathbb{E}[e^{i \langle t, x \rangle}]$</p><p> [Exercise] let $Y \sim N(0,1)$, then $\phi_Y(t) &#x3D; e^{-\frac{t^2}{2}}$</p><p> Note that for any r.v. $Z$, the c.f. of $bZ +d$ is </p><p> $$<br> \phi_{bZ+d}(t) \&#x3D; \mathbb{E}[e^{i(bZ+d)t}] \&#x3D; \mathbb{E}[e^{idt+ibtZ}] \&#x3D; e^{idt}\mathbb{E}[e^{ibtZ}] \&#x3D;e^{idt}\phi_Z(bt)<br> $$</p><p> ‚áí $\xi \sim N(\mu, \sigma^2)$, then $\phi_{\xi}(t)&#x3D;e^{i\mu t}e^{-\frac{t^2\sigma ^2}{2}}$ (Indeed, $\frac{\xi-\mu}{\sigma} \sim N(0,1)$, so $\xi &#x3D; \sigma Z +\mu$ where $Z$ is standard normal r.v.)</p></li></ol><p>Observe that $\langle t, x \rangle$ has normal distribution: $t^Tx &#x3D; t^Ta + (t^TB)Y$ ‚Äî a linear transformation of standard normal distribution</p><p>$\mathbb{E}[t^TX] &#x3D; t^Ta$</p><p>$Var(t^TX) &#x3D; \mathbb{E}[(t^TBY)^2] &#x3D; \mathbb{E}[t^TBYY^TB^Tt] &#x3D; t^TBB^Tt  &#x3D; t^T\Sigma_X t$ (we can drop the $\mathbb{E}$, because $t, B$ are both constant)</p><h3 id="Characteristic-Function-of-multivariate-normal"><a href="#Characteristic-Function-of-multivariate-normal" class="headerlink" title="Characteristic Function of multivariate normal"></a>Characteristic Function of multivariate normal</h3><aside>üí° Properties of Characteristic function for single normal random variable: $X$<p>$\phi_X(t) &#x3D; E[e^{itx}] &#x3D; E[cos(tx)] + i E[sin(tx)]$</p><p>$\phi_x(t) &#x3D; \int_R e^{itx} p(x) dx$</p><p>For standard normal r.v. $Y \sim N(0,1)$, $\phi_Y(t) &#x3D; e^{-\frac{t^2}{2}}$</p><p>For any r.v. $\xi \sim N(\mu, \sigma^2)$, $\phi_{\xi}(t)&#x3D;e^{i\mu t}e^{-\frac{t^2\sigma ^2}{2}}$</p></aside><aside>üí° Properties of Characteristic function for multivariate normal distribution:<p>For random vector $X \in R^n$, $\phi_X(t) &#x3D; \mathbb{E}[e^{i \langle t, X \rangle}]$</p><p>For any multivariate normal variable $X \in R^n$, $\phi_X(t) &#x3D; \mathbb{E}[e^{i \langle t,x \rangle}] &#x3D; e^{i \langle t, \mathbb{E}[x] \rangle - \frac{\langle \Sigma_X t, t \rangle}{2}}$</p></aside><p>For any multivariate normal variable $X \in R^n$, $\phi_X(t) &#x3D; \mathbb{E}[e^{i \langle t,x \rangle}] &#x3D; e^{i \langle t, \mathbb{E}[x] \rangle - \frac{\langle \Sigma_X t, t \rangle}{2}}$</p><p>Proof:</p><p>let $X &#x3D; a+BY$ be multivariate normal, then </p><p>$$<br>\phi_X(t) &#x3D; \mathbb{E}[e^{i \langle t,x \rangle}]<br>\&#x3D; \mathbb{E}[e^{i \langle t,a \rangle + i \langle t, bY \rangle}]<br>\&#x3D; e^{i \langle t,a \rangle}\mathbb{E}[e^{i \langle t, BY \rangle}]<br>\ &#x3D; e^{i \langle t,a \rangle}\mathbb{E}[e^{i \langle B^Tt, Y \rangle}]<br>$$</p><p>For $\langle B^Tt, Y \rangle ,$$\langle B^Tt, Y \rangle &#x3D; \sum_{j&#x3D;1}^n (B^Tt)_j Y_j$ ‚áí normal distribution </p><p>Since any linear combination of independent normal r.v. is still normal (can be proved using characteristic function)</p><p>Therefore, $\mathbb{E}[ \langle B^Tt, Y \rangle] &#x3D; 0$ and $Var(\langle B^Tt, Y \rangle) &#x3D; ||B^Tt||_2^2$ (since sum of variance &#x3D; variance of sum and $Var((B^Tt)_j Y_j) &#x3D; (B^Tt_j)^2 Var(Y_j) &#x3D; (B^Tt_j)^2$)</p><p>‚áí $\langle B^Tt, Y \rangle \sim N(0, ||B^Tt||_2^2)$ which can be regard as $||B^Tt||_2Y$ where $Y$ is a standard random variable</p><p>‚áí $\mathbb{E}[e^{i \langle B^T t, Y \rangle}] &#x3D; \mathbb{E}[e^{i(t^TB)Y}]&#x3D; e^{-\frac{||B^Tt||_2^2}{2}}$ </p><p>Since $||B^Tt||_2^2 &#x3D; \langle B^Tt, B^Tt \rangle &#x3D; \langle t, BB^Tt \rangle &#x3D; \langle t, \Sigma_x t \rangle &#x3D; \langle \Sigma_x t, t \rangle$, </p><p>$\mathbb{E}[e^{i \langle B^T t, Y \rangle}] &#x3D; e^{-\frac{||B^Tt||_2^2}{2}} &#x3D; e^{-\frac{\langle \Sigma_X t, t \rangle}{2}}$</p><p>Therefore, the final characteristic function of a multivariate normal distribution is $\phi_X(t) &#x3D; \mathbb{E}[e^{i \langle t,x \rangle}] &#x3D; e^{i \langle t, \mathbb{E}[x] \rangle - \frac{\langle \Sigma_X t, t \rangle}{2}}$</p><aside>üí° Proposition:<p>$X &#x3D; (X_1, ‚Ä¶, X_k)$ has multivariate normal distribution $\sim N(a, \Sigma_X)$</p><p>‚Üî¬† $\forall Z &#x3D; (Z_1, ‚Ä¶ ,Z_k) \in R^k$, $\langle Z, X \rangle \sim N(Z^Ta, Z^T \Sigma_x Z)$</p></aside><p>Proof:</p><ol><li><p>forward direction: </p><p> $X \sim N(a, \Sigma_X)$ means that $X &#x3D; a + BY$ where $BB^T&#x3D;\Sigma_X$ and $Y$ is a vector of i.i.d. standard r.v.s</p><p> $Z^TX &#x3D; Z^T(a+BY) &#x3D; Z^Ta + Z^TBY$, which is a linear combination of independent normal distributions, so it must be normal distributions</p><p> or can be regard as a linear transformation of $Y$</p><p> $E[Z^TX] &#x3D; Z^Ta$</p><p> $Var(Z^TX) &#x3D; (Z^TB)(Z^TB)^T &#x3D; Z^TBB^TZ &#x3D; Z^T \Sigma_XZ$</p><p> ‚áí $\langle Z, X \rangle \sim N(Z^Ta, Z^T \Sigma_x Z)$</p></li><li><p>backward direction:</p><p> Assume that $\langle Z, X \rangle \sim N(Z^Ta, Z^T \Sigma_x Z) \forall Z\in R^k, Z \ne 0$</p><p> use the characteristic function of multivariate normal distribution: $\phi_X(t) &#x3D; \mathbb{E}[e^{i \langle t,x \rangle}] &#x3D; e^{i \langle t, \mathbb{E}[x] \rangle - \frac{\langle \Sigma_X t, t \rangle}{2}}$</p><p> c.f. of $Z^TX$ is $\phi_{Z^TX}(t) &#x3D; \mathbb{E}[e^{i(Z^TX)t}] &#x3D; e^{iZ^Tat} e^{-\frac{t^2Z^T\Sigma_XZ}{2}}$</p><p> take $t&#x3D;1$ ‚áí $\mathbb{E}[e^{i(Z^Tx)}] &#x3D; e^{i(Z^Ta)} e^{-\frac{Z^T\Sigma_X Z}{2}}$ matches the c.f. of multivariate normal distribution</p></li></ol><h3 id="Example-of-p-d-f-DNE-that‚Äôs-when-we-can-only-use-characteristic-function"><a href="#Example-of-p-d-f-DNE-that‚Äôs-when-we-can-only-use-characteristic-function" class="headerlink" title="Example of p.d.f. DNE (that‚Äôs when we can only use characteristic function)"></a>Example of p.d.f. DNE (that‚Äôs when we can only use characteristic function)</h3><p>Question: $X &#x3D; a +BY$, $a \in R^k$, $B \in R^{k \times m}$, $Y \sim N (0,1)$. What if $rank(B) &lt; k$?</p><p>Remainder: rank &#x3D; # of max linearly independent columns</p><p>Note that $\Sigma_X &#x3D; BB^T$ </p><p>‚áí $det(\Sigma_x) &#x3D; 0$ </p><p>‚Üî$\exist Y \in R^k$  s.t. $\Sigma_X Y &#x3D;0$ </p><p>‚áí $Var(Y^TX)&#x3D; Y^T\Sigma_X Y &#x3D; 0$ </p><p>‚áí $Y^TX &#x3D; Y^Ta$ ($a$ is some constant vector)</p><p>‚áí coordinates of $X$ are linearly dependent</p><p>($rank(B) &lt; k \lrarr x$ ‚Äúlives‚Äù in a subspace of $R^k$  &#x3D; $X$ does not have a p.d.f.)</p><p><strong>Case</strong> k&#x3D;2, $rank(B)&#x3D;1$:</p><p>$X&#x3D;(x_1,x_2) \in R^2$, then $X$ belong to some line $ax+by+c$  in this plane</p><p>$X$ does not have a p.d.f. in this case</p><p>Proof:</p><p>Assume that p.d.f. $p_x$ exists. WLOG, assume that $p_x$ is bounded</p><p>Pick some area $R$ s.t. $Pr(x\in R) &gt; 0$</p><p>$Pr(x\in R) &#x3D; \int_R Pr(x) dx \le \sup_{x \in R} Pr(x) \times volume(R)$</p><p>assume $Area(R) &#x3D; r_1 \times r_2$</p><p>$Pr(x \in R)$ must be the same $\forall r_2$</p><p>but if $r_2 \rarr 0, \sup_{x\in R} Pr(x) \times volume(R) \rarr 0$</p><p>$\lrarr Pr(x \in R) \rarr 0$</p><p>‚áí $Pr(x \in R)$ does not exists</p><p>‚áí contradiction</p><p>‚áí $P_x$ does not exist</p><p><img src="/./img/MATH545/Untitled%203.png" alt="Untitled"></p><p>example for p.d.f. does not exist:</p><p> $X \sim N(0,1)$, let a vector be $(X, 1)$ ‚áí there is density on the line, but no density in the 2D space</p><h3 id="Deriving-pdf-of-a-full-rank-random-vector"><a href="#Deriving-pdf-of-a-full-rank-random-vector" class="headerlink" title="Deriving pdf of a full-rank random vector"></a>Deriving pdf of a full-rank random vector</h3><p>Now assume that $rank(B) &#x3D; k$ (in particular, imply $m \ge k$, $B \in R^{k \times m}$)</p><p>Let us derive the p.d.f. of $X$</p><p>Remainder:</p><p>let $Z \in R^k$ be a random vector with p.d.f. $P_Z$, assume that $Z &#x3D; g(x)$ for some one-on-one function $g$, what is the p.d.f. of X?</p><p>$Pr(Z \in A) &#x3D; \int_A P_Z(z) dz$</p><p>‚áí $Pr(x \in g^{-1}(A)) &#x3D; \int_{g^{-1}(A)}P_Z(g(x)) |D_{g(x)}|dx$,  where $|D_{g(x)}|$ is the Jacobian matrix of $g(x)$</p><p>‚áí p.d.f. of x (represented by $g(x)$) is: $P_Z(g(x)) |D_{g(x)}|$</p><p>we need to further derive a closed form for this equation</p><p>Step 1: Take $Z \in R^k$ to be standard normal</p><p>$Z &#x3D; (Z_1, ‚Ä¶ Z_k)$, $Z_1, ‚Ä¶ ,Z_k$ be independent $N(0,1)$</p><p>‚áí $P_Z(Z_1, ‚Ä¶, Z_k) &#x3D; P(Z_1) ‚Ä¶ P(Z_k), P(Z_i) &#x3D; \frac{1}{\sqrt {2\pi}}e^{-\frac{z^2}{2}}$</p><p>‚áí $P_Z(Z_1, ‚Ä¶ , Z_k) &#x3D; \frac{1}{(\sqrt{2\pi})^k}e^{-\sum_{i}^k\frac{z_i^2}{2}} &#x3D; \frac{1}{(\sqrt{2\pi})^k}e^{-\frac{||z||_2^2}{2}}$</p><p>Step2: Define $X \sim N(0, \Sigma_X), \Sigma_X &gt; 0$ (normalize $X$)</p><p>Note that $\Sigma_X &#x3D; U \Lambda U^{T}$ where $U$ is a matrix whose columns are eigenvectors and $\Lambda &#x3D; diag(\lambda_1, ‚Ä¶ ,\lambda_k)$</p><p>To normalize $X$, define $Z &#x3D; \Lambda^{-\frac{1}{2}}U^TX$ ($Z$ is a multivariate normal since it is a linear combination of multivariate normal)</p><p>$E[Z] &#x3D; 0$</p><p>$\Sigma_Z &#x3D; E[ZZ^T] &#x3D; E[\Lambda^{-\frac{1}{2}}U^TX X^T U \Lambda^{-\frac{1}{2}}] &#x3D; \Lambda^{-\frac{1}{2}}U^T E[XX^T]U \Lambda^{-\frac{1}{2}} &#x3D; \Lambda^{-\frac{1}{2}}U^T U\Lambda U^TU \Lambda^{-\frac{1}{2}}$</p><p>since $U^TU &#x3D; I$ (as $U$ is orthogonal matrix), $\Sigma_Z &#x3D; I$</p><p>‚áí $Z \sim N(0,I_k)$ in this case</p><p>‚áí $P_X(X_1, ‚Ä¶,X_K) &#x3D; \frac{1}{(2\pi)^{\frac{k}{2}}}\exp(-||\Lambda^{-\frac{1}{2}}U^TX||_2^2 X||)$ and what I can‚Äôt understand whats on the blackboard</p><p><img src="/./img/MATH545/Untitled%204.png" alt="Untitled"></p><p><img src="/./img/MATH545/Untitled%205.png" alt="Untitled"></p><p><img src="/./img/MATH545/Untitled%206.png" alt="Untitled"></p><h3 id="Chapter-2-Hilbert-Space-not-updated-yet"><a href="#Chapter-2-Hilbert-Space-not-updated-yet" class="headerlink" title="Chapter 2 Hilbert Space (not updated yet)"></a>Chapter 2 Hilbert Space (not updated yet)</h3><p>Definition: A <strong>pre-Hilbert space</strong> is a vector space equipped with an inner product $\langle \cdot ; , ; \cdot \rangle: \mathbb{H} \times \mathbb{H} \rarr \mathbb{C} $ that satisfies: </p><ol><li><p>$\langle x ; , ; y \rangle &#x3D; \overline{ \langle y ; , ; x \rangle}$</p></li><li><p>$ \langle \alpha x ; , ; y \rangle &#x3D; \alpha \langle x ; , ; y \rangle$</p></li><li><p>$ \langle x+y ; , ; z \rangle &#x3D; \langle x ; , ; z \rangle + \langle y ; , ; z \rangle$</p></li><li><p>$\langle x ; , ; x \rangle \ge 0 $ and $\langle x ; , ; z \rangle &#x3D; 0 \iff x &#x3D; 0 $</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>Study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>time series</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
